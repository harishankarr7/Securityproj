The widespread adoption of machine learning models in various applications raises concerns about the privacy of sensitive information contained in the training data. Recent studies have shown that standard machine learning models can memorize sensitive information from their training data, leaving them vulnerable to reconstruction attacks. In this paper, we extend the work of Borja Balle, Giovanni Cherubin and Jamie Hayes by re-verifying their experiments on reconstruction attacks against standard MNIST and CIFAR-10 classifiers. Our extension includes the evaluation of the robustness of these classifiers against reconstruction attacks in the presence of different types of hyper-parameters. We also investigate the impact of data and computational efficiency on the performance of the attack. Our results confirm the effectiveness of reconstruction attacks on standard machine learning models and highlight the importance of protecting against these attacks. Furthermore, we propose and test the effectiveness of an approach that uses differential privacy training with large values of Ïµ as a defense mechanism against reconstruction attacks. Our work provides insights into the development of accurate and resilient machine learning models that can better protect the privacy of sensitive data.